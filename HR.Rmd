---
title: "HR"
author: "Suchitra"
date: "3/21/2017"
output:
  pdf_document: default
  html_document: default
---
#Questions:
#Why are our best and most experienced employees leaving prematurely? 
#Which Valuable employee will leave next

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
```



```{r}
#Code the missing values as NA
hr_data <- read.csv("HR_comma_sep.csv", header = T, na.strings = c(""))
sapply(hr_data, function(x) sum(is.na(x))) #No missing values present in the data

#Lets explore this dataset
names(hr_data)
#Structure of the dataset
str(hr_data)
```
Finding the structure of the dataset gives us an information about the following:
Type of dataset: Data Frame
Number of variables and records
Data Type of the variables: Num, int, factor
Target variable : left

```{r}
table(hr_data$left)

#Satisfaction level of people who left
ggplot(data=hr_data, aes(x=factor(hr_data$left),y=hr_data$satisfaction_level))+
  geom_boxplot(aes(color=factor(hr_data$left)))+
  xlab("Left")+
  ylab("Satisfaction level")
by(hr_data$satisfaction_level, hr_data$left, summary)

```
Until now, 23.8% of the people have left the company.

The satisfaction level of employees who left the company(median= 0.44) is much lower than that of the employees who stayed(0.69). This may indicate that the employees ae leaving the company due to dissatisfaction in their work.


```{r}
#Evaluation
ggplot(data=hr_data, aes(hr_data$last_evaluation))+
  geom_density(aes(group= factor(hr_data$left),fill=factor(hr_data$left)))+
  xlab("Left")+
  ylab("Last Evaluation")
by(hr_data$last_evaluation, hr_data$left, summary)


#Relationship between satisfaction levels and last_evaluation.
ggplot(aes(hr_data$last_evaluation, hr_data$satisfaction_level), data=hr_data)+
  geom_point(alpha=1/10, col="red")+
  facet_wrap(~hr_data$left)
```

We can see two peaks of evaluation scores for people who left and this indicates that most people who left are extremely high or extremely low performers.

The plot for satisfaction levels and last evaluation is tells us that these both factors might be related. For the  employees that left the company, satisfaction levels are lesser as compared to the ones staying back. 
We can see two distinct pattern for the employees who left the company, one where the evaluation is very high (high performers), but the satisfaction level is very less. Other where the satisfaction and evaluation are on the lower side. 



```{r}
#Average_monthly_hours
ggplot(data=hr_data, aes(x=factor(hr_data$left),y=hr_data$average_montly_hours))+
  geom_boxplot(aes(color=factor(hr_data$left)))+
  xlab("Left")+
  ylab("Average Monthly Hours")


```

Average monthly hours of people who left is higher than that of people who stayed.

```{r}
#Time spend in the company
ggplot(data=hr_data, aes(x=factor(hr_data$left),y=hr_data$time_spend_company))+
  geom_boxplot(aes(color=factor(hr_data$left)))+
  xlab("Left")+
  ylab("Time spend in the company")
```

People who left the company have a much higher tenure as compared to the ones who stayed.


```{r}
#Salary
table(hr_data$salary)
by(hr_data$salary, hr_data$left, table)
```

6.6% of people from higher salary range left, 29.68% from low salary range left, 20.4% from medium salary range left. Thus, its clear that people from lower salary range tend to leave the company.


```{r}
#Number of projects
by(hr_data$number_project,hr_data$left,table)

```

Maximum number of people who did not leave, seem to work on 3 or 4 projects in the comapny.Maximum number of people who left seem to have worked in 2 projects or higher numbers like 6 or 7 in the comapny.


```{r}
#Promotion in last 5 years
table(hr_data$promotion_last_5years)
by(hr_data$promotion_last_5years,hr_data$left, table)

```

Only 2.2% of the people in the company were promoted in the last 5 years. 
2.7% of people who stayed got the promotion, whereas only 0.5% of people who left had got a promotion.

```{r}
#Sales
x<- table(hr_data$sales, hr_data$left)
by(hr_data$sales, hr_data$left, table)
ggplot(aes(hr_data$sales), data=hr_data)+
  geom_bar(aes(fill=factor(hr_data$left)))+
  xlab("Sales")
```


```{r}
#Satisfaction level vs salary
ggplot(aes(hr_data$salary,hr_data$satisfaction_level), data=hr_data)+
  geom_raster(aes(fill=hr_data$left))

```

Important observations/Insights:

People who left the company seem to be less satisfied as compared to the ones staying back. 
Higher working hours might be one of the reasons for the people to leave the company.
People who left the company seem to have higher tenure. This may imply that they are looking for better opportunities or looking for a change in job. 
People having low salaries seem to have left the company in large numbers, this may be due to their dissatisfaction due to lower salaries or higher opportunities in the market for lower levels.
People who left seem to have extremely high or low performance evaluation.This may mean that they are not happy in the job and are leaving or they are overqualified and are looking for better opportunities. 
Promotion might be an important factor in a person's decision to leave or stay back.



#Let us find the bivariate relationship present in the data. First lets find the correlation between the output variable i.e left and all other variables.
```{r}
#Correlations are performed on numeric values and hence converting sales and salary to numeric value.
hr_data$sales <- as.numeric(hr_data$sales)
hr_data$salary <- as.numeric(hr_data$salary)
x <- cor(x=hr_data[,1:10], y= hr_data[,1:10])
```
We find the correlation between all the variables to examine the relationship between the variables themselves.Correlation shows how strongly two variables are related. A positive correlation shows that as 1 variable increases the other increases too, while a negative correlation shows that a one variable decreases the other decreases too.

Satisfaction level is the strongest correlated variable with left.
Performance is correlated with average monthly hours and number of projects.
Number of projects is correlated with average monthly hours.

#Relationship between employees leaving and other factors
```{r}
#Obtaining the train and test dataset
sample <- floor(0.7*nrow(hr_data))
set.seed(100)
hr_indices <- sample(seq_len(nrow(hr_data)), size=sample)

#Load the train and test data
hr_train <- hr_data[hr_indices,]
hr_test <- hr_data[-hr_indices,]

#Fitting a Binomial Logistic regression model for leaving the company
model <- glm(hr_data$left ~., family = binomial(link="logit"), data=hr_data)
summary(model)


```
The p value for all the variables are statistically significant.
Satisfaction level, Number of projects, work accident, promotion and sales(considering all the coefficients for sales), these varaibles have a negative relationship with a person leaving the company.

#Prediction
```{r}

hr_predict <- predict(model,type = "response", hr_test)
hr_predict <-ifelse(hr_predict > 0.5,1,0)

Error <-mean(hr_predict != hr_test$left)
print(paste('Accuracy', 1-Error))



```
After performing out of sample validation using the test data, we get the the accuracy of this model to be 0.77 which is high. Thus, we can say that this model is a good fit to our data.


#Performance of the logistic regression model
```{r}
#install.packages("ROCR")
library(ROCR)
hr_predict1 <- predict(model,type = "response", hr_test)
pr <- prediction(hr_predict1, hr_test$left)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc


```
We plot an ROC curve to get the Area under the curve(AUC), which is an indication of how well the model performs. Thue AUC comes out to be 0.8. Thus this tells us that there is scope of improvement to this model.

We try to model this data on a random forest algorithm, to compare it with the logistic regression model and see if this model has a better fit as compared to the previous.

#Random forest
```{r}

library(randomForest)

hr_rf <- randomForest(as.factor(hr_train$left)~.,hr_train, importance=TRUE, ntree=1000,method='class')

pred <- predict(hr_rf,hr_test)
table(pred, hr_test$left)

accuracy<- (3421+1043)/nrow(hr_test)
accuracy


```

As we can see the random forest mode gives an accuracy of 0.992, which is very high. This model fits our data much better than the logistic regression model.

#Extensive Logitic Regression:
```{r}
# We start the model with a single explanatory variable
var1 <- glm(hr_data$left~ hr_data$satisfaction_level, data=hr_data, family = binomial())
summary(var1)

# 2nd variable
var2 <- glm(hr_data$left ~ hr_data$satisfaction_level+hr_data$last_evaluation, data=hr_data, family = binomial())
summary(var2)

var3 <- glm(hr_data$left ~ hr_data$satisfaction_level+hr_data$last_evaluation+ hr_data$number_project, data=hr_data, family = binomial())
summary(var3)

var4 <- glm(hr_data$left ~ hr_data$satisfaction_level+hr_data$last_evaluation+ hr_data$number_project+ hr_data$average_montly_hours, data=hr_data, family = binomial())
summary(var4)

var5 <- glm(hr_data$left ~ hr_data$satisfaction_level+hr_data$last_evaluation+ hr_data$number_project+ hr_data$average_montly_hours + hr_data$time_spend_company, data=hr_data, family = binomial())
summary(var5)

#
var6 <- glm(hr_data$left ~ hr_data$satisfaction_level+hr_data$last_evaluation+ hr_data$number_project+ hr_data$average_montly_hours + hr_data$time_spend_company +hr_data$Work_accident, data=hr_data, family = binomial())
summary(var6)

var7 <- glm(hr_data$left ~ hr_data$satisfaction_level+hr_data$last_evaluation+ hr_data$number_project+ hr_data$average_montly_hours + hr_data$time_spend_company +hr_data$Work_accident+ hr_data$promotion_last_5years, data=hr_data, family = binomial())
summary(var7)

var8 <- glm(hr_data$left ~ hr_data$satisfaction_level+hr_data$last_evaluation+ hr_data$number_project+ hr_data$average_montly_hours + hr_data$time_spend_company +hr_data$Work_accident+ hr_data$promotion_last_5years + hr_data$sales, data=hr_data, family = binomial())
summary(var8)

var8 <- glm(hr_data$left ~ hr_data$satisfaction_level+hr_data$last_evaluation+ hr_data$number_project+ hr_data$average_montly_hours + hr_data$time_spend_company +hr_data$Work_accident+ hr_data$promotion_last_5years + hr_data$sales +hr_data$salary, data=hr_data, family = binomial())
summary(var8)


```

